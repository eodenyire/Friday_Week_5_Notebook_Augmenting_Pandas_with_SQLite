# -*- coding: utf-8 -*-
"""Friday Week 5 Notebook: Augmenting Pandas with SQLite - Emmanuel Odenyire Anyira

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fD8W6XPP_uh8Pby-rMeSXRjYpMkMHXeV

# Project Notebook: Augmenting Pandas with SQLite

## Question 1: Introduction

In this session, we explored a few different ways to work with larger datasets in pandas. In this guided project, we'll practice using some of the techniques we learned to analyze startup investments from Crunchbase.com.

Every year, thousands of startup companies raise financing from investors. Each time a startup raises money, we refer to the event as a fundraising round. Crunchbase is a website that crowdsources information on the fundraising rounds of many startups. The Crunchbase user community submits, edits, and maintains most of the information in Crunchbase.

In return, Crunchbase makes the data available through a Web application and a fee-based API. Before Crunchbase switched to the paid API model, multiple groups crawled the site and released the data online. Because the information on the startups and their fundraising rounds is always changing, the data set we'll be using isn't completely up to date.

Throughout this project, we'll practice working with different memory constraints. In this step, let's assume we only have 10 megabytes of available memory. While crunchbase-investments.csv (https://bit.ly/3BPcobU) consumes 10.3 megabytes of disk space, we know from earlier lessons that pandas often requires 4 to 6 times amount of space in memory as the file does on disk (especially when there's many string columns).


**Tasks**

* Because the data set contains over 50,000 rows, you'll need to read the data set into dataframes using 5,000 row chunks to ensure that each chunk consumes much less than 10 megabytes of memory.
* Across all of the chunks, become familiar with:
1. Each column's missing value counts.
2. Each column's memory footprint.
3. The total memory footprint of all of the chunks combined.
4. Which column(s) we can drop because they aren't useful for analysis.

*Reading the Crunchbase data in chunks of 5000 rows, then print the first 5 rows of each chunk. We need to see the data and make sure that the chunks are being read correctly by pandas. The encoding parameter is set to 'unicode_escape' to properly handle any special characters in the data.*
"""

# Your code goes here
url = "https://bit.ly/3BPcobU"
#
import pandas as pd
for crunchbase in  pd.read_csv(url, encoding='unicode_escape', chunksize = 5000):
  print(crunchbase.head(5))

"""*We want to return the memory usage of each column in the current chunk of the Crunchbase data. The deep parameter has been set to True, which means that the memory usage of all the elements in each column will be calculated, including any nested elements. The result is a pandas Series object, where the index is the column names and the values are the memory usage of each column in bytes.*"""

crunchbase.memory_usage(deep = True)

"""*We intend to calculate the total memory footprint of all the chunks of the Crunchbase data by summing up the memory usage of each chunk. The memory_usage() method is called on each chunk to get its memory usage, and the .sum() method is used to get the sum of all the memory usage of each column in the chunk. The result is then divided by 1024 * 1024 to convert it from bytes to megabytes. Finally, the memory usage of each chunk is appended to the memory_footprints list, and the sum of all the values in memory_footprints is printed.*
                                                                                                                                                   *This gives us the combined output*
"""

memory_footprints = []
crunchbase_iter = pd.read_csv(url, encoding='unicode_escape', chunksize = 5000)
for chunk in crunchbase_iter:
    memory_footprints.append(crunchbase.memory_usage(deep=True).sum()/(1024*1024))
print(sum(memory_footprints))

"""*To remove the unnecessary columns, we read the entire Crunchbase data into a single pandas DataFrame, and then drop several columns that aren't useful for analysis. The drop method is used to remove the specified columns, which are listed in the argument passed to the columns parameter.
The axis parameter is set to 1 to indicate that we are dropping columns (as opposed to rows), and the result of this operation is 
a new DataFrame with the specified columns removed. Note that this code doesn't assign the result of the drop operation back to the crunchbase DataFrame. To make the change permanent, you should re-assign the result to crunchbase like this: crunchbase = crunchbase.drop(['investor_country_code', 'investor_state_code','investor_category_code', 'company_city','raised_amount_usd','investor_city', 'company_state_code','company_category_code',], axis = 1).*
"""

crunchbase = pd.read_csv(url, encoding='unicode_escape')
crunchbase.drop(['investor_country_code', 'investor_state_code','investor_category_code', 'company_city','raised_amount_usd','investor_city','company_state_code','company_category_code',], axis = 1)

"""*We then print the number of missing values for each column in the crunchbase DataFrame. The isnull() method is used to create a DataFrame of the same shape as crunchbase with True values where there are missing values and False values where there are no missing values. The sum method is then applied to this DataFrame to count the number of True values in each column, which gives the number of missing values in each column. The result is a pandas Series object, where the index is the column names and the values are the number of missing values in each column.*"""

print( crunchbase.isnull().sum())

"""## Question 2: Selecting Data Types

Now that we have a good sense of the missing values, let's get familiar with the column types before adding the data into SQLite.

**Tasks**

* Identify the types for each column.
* Identify the numeric columns we can represent using more space efficient types.
For text columns:
* Analyze the unique value counts across all of the chunks to see if we can convert them to a numeric type.
* See if we clean clean any text columns and separate them into multiple numeric columns without adding any overhead when querying.
* Make your changes to the code from the last step so that the overall memory the data consumes stays under 10 megabytes.

*Let us use the info() method of a pandas dataframe to display information about the dataframe including:
i. The number of rows and columns
ii. The data types of each column
iii. The memory usage of the dataframe (by column)
This information is useful for understanding the structure of the data and identifying any issues with the data types, such as data being stored as the wrong data type, or columns that take up more memory than necessary.*
"""

# Your code goes here

print(crunchbase.info())

"""*First we create a list numeric that includes the data types corresponding to numeric data (integer and floating point types). Then we use the select_dtypes method to extract the columns in the dataframe crunchbase that have data types included in numeric, and stores the resulting column names in a variable numeric_cols. Finally, the numeric_cols variable is printed to display the names of the numeric columns.*"""

numeric= ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numeric_columns = crunchbase.select_dtypes(include=numeric).columns
numeric_columns

"""*We then iterate over the chunks of the crunchbase-investments.csv file and for each chunk, it calculates the value counts of the unique values in each column of the chunk. The value counts are stored in the chunk_vc variable and printed for each iteration.*"""

crunchbase_iter = pd.read_csv(url, encoding='unicode_escape', chunksize = 5000)
for crunchbase in crunchbase_iter:
    chunk_vc = crunchbase.value_counts()
    print(chunk_vc)

"""## Question 3: Loading Chunks Into SQLite

Now we're in good shape to start exploring and analyzing the data. The next step is to load each chunk into a table in a SQLite database so we can query the full data set.

**Tasks**

1. Create and connect to a new SQLite database file.
2. Expand on the existing chunk processing code to export each chunk to a new table in the SQLite database.
3. Query the table and make sure the data types match up to what you had in mind for each column.

*We read the csv file in chunks of 5000 rows at a time using pandas' read_csv method. We then connect to a SQLite database with the connection object "conn", and for each chunk, we write the chunk to an SQLite table called "exhibitions". After writing each chunk, we run a SQL query "PRAGMA table_info(exhibitions);" to retrieve the table information for the "exhibitions" table, and store the results in a Pandas dataframe called "results_df". Finally, we prints the "results_df" dataframe.*
"""

# Your code goes here
# Your code goes here
import sqlite3
import pandas as pd
crunchbase_iter = pd.read_csv(url, encoding='unicode_escape', chunksize = 5000)
conn = sqlite3.connect('crunchbase_iter')
for chunk in crunchbase_iter:
    chunk.to_sql("exhibitions", conn, if_exists='append', index=False)
    results_df = pd.read_sql('PRAGMA table_info(exhibitions);', conn)
print(results_df)

"""## Question 4: Next Steps

Now that the data is in SQLite, we can use the pandas SQLite workflow we learned in the last lesson to explore and analyze startup investments. Remember that each row isn't a unique company, but a unique investment from a single investor. This means that many startups will span multiple rows.

Use the pandas SQLite workflow to answer the following questions:

* What proportion of the total amount of funds did the top 10% raise? What about the top 1%? Compare these values to the proportions the bottom 10% and bottom 1% raised.
* Which category of company attracted the most investments?
* Which investor contributed the most money (across all startups)?
* Which investors contributed the most money per startup?
* Which funding round was the most popular? Which was the least popular?

Here are some ideas for further exploration:

* Repeat the tasks in this project using stricter memory constraints (under 1 megabyte).
* Clean and analyze the other Crunchbase data sets from the same GitHub repo.
* Understand which columns the data sets share, and how the data sets are linked.
* Create a relational database design that links the data sets together and reduces the overall disk space the database file consumes.

Use pandas to populate each table in the database, create the appropriate indexes, and so on.

# What proportion of the total amount of funds did the top 10% raise? What about the top 1%? Compare these values to the proportions the bottom 10% and bottom 1% raised.

Total amount of funds raised by the top 10% and top 1%

*We execute an SQL query on the crunchbase_iter SQLite database, which we have been reading the data from a CSV file into in chunks and storing in the database. We then select the raised_amount_usd column and count the number of occurrences of each unique value in the raised_amount_usd column and store the results in the eid_counts DataFrame. The query groups the data by raised_amount_usd and orders the results by counts in descending order. Finally, it prints the first 10 rows of the eid_counts DataFrame.*
"""

# Your code goes here
question = 'select raised_amount_usd, count(*) as counts from exhibitions group by raised_amount_usd  order by counts desc;'
eid_counts = pd.read_sql(question, conn)
print(eid_counts[:10])

"""Total amount of funds raised by the bottom 10% and bottom 1%

*We connect to the SQLite database and use a pandas DataFrame to store the results of the SQL query. We store the query in the variable 'question', which selects the 'raised_amount_usd' column and counts the number of rows for each unique value in that column. The result is stored in a pandas DataFrame called 'eid_counts', which is sorted in ascending order by the count of rows for each unique 'raised_amount_usd' value. Finally, we print the first 10 rows of the 'eid_counts' DataFrame.*
"""

question = 'select raised_amount_usd , count(*) as counts from exhibitions group by raised_amount_usd  order by counts asc;'
eid_counts = pd.read_sql(question, conn)
print(eid_counts[:10])

"""# Which category of company attracted the most investments?

Top 5 companies with the largest investments

*We execute an SQL query using the pd.read_sql function from the pandas library. The query is selecting the company_category_code column and counting the number of occurrences of each unique value in the column. The result is grouped by the raised_amount_usd column and ordered by the count in descending order to identify the best companies from the top. The resulting data is then displayed with the first 10 rows printed to the console using the print function.*
"""

query = 'select company_category_code , count(*) as counts from exhibitions group by raised_amount_usd  order by counts desc;'
eid_counts = pd.read_sql(query, conn)
print(eid_counts[:5])

"""# Which investor contributed the most money (across all startups)?

*We execute an SQL query on the SQLite database created, selecting the "investor_name" column and counting the number of occurrences of each unique value of investor name (grouped by the "raised_amount_usd" and "company_name" columns). The result is ordered by the count in descending order. Finally, we print the first 10 rows of the resulting DataFrame.*
"""

q = 'select investor_name , count(*) as counts from exhibitions group by raised_amount_usd and company_name order by counts desc;'
eid_counts = pd.read_sql(q, conn)
print(eid_counts[:10])

"""# Which investors contributed the most money per startup?

*We execute an SQL query on a database connection conn and retrieve the results of the query into a pandas dataframe eid_counts. The query selects the investor_name, raised_amount_usd, and the count of records with the same raised_amount_usd, and groups the results by raised_amount_usd and orders them in descending order. We then print the first 10 rows of the resulting dataframe.*
"""

query = 'select investor_name,raised_amount_usd , count(*) as counts from exhibitions group by raised_amount_usd order by raised_amount_usd  desc;'
eid_counts = pd.read_sql(query, conn)
print(eid_counts[:10])

"""# Which funding round was the most popular? Which was the least popular?

Most popular funding round

*We are executing an SQL query and retrieving the results of the query into a pandas dataframe eid_counts. The query selects the funding_round_type and the count of records with the same funding_round_type, and groups the results by funding_round_type and orders them in descending order by the count of records. We then prints the first 10 rows of the resulting dataframe.*
"""

query = 'select funding_round_type , count(*) as counts from exhibitions group by funding_round_type order by counts desc;'
eid_counts = pd.read_sql(query, conn)
print(eid_counts[:10])

"""Least popular funding round

*We are executing an SQL query on a database and retrieving the results of the query into a pandas dataframe eid_counts. The query selects the funding_round_type and the count of records with the same funding_round_type, and groups the results by funding_round_type and orders them in ascending order by the count of records. We then prints the first 10 rows of the resulting dataframe.*
"""

query = 'select funding_round_type , count(*) as counts from exhibitions group by funding_round_type order by counts asc;'
eid_counts = pd.read_sql(query, conn)
print(eid_counts[:10])